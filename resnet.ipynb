{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 135\n",
    "pre_epoch = 0\n",
    "batch_size = 128\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transfrms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='data',train=True,download=True,transform=transform_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='data',train=False,download=True,transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=10,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=lr,momentum=0.9,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    子 module: Residual Block ---- ResNet 中一个跨层直连的单元\n",
    "    \"\"\"\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 如果输入和输出的通道不一致，或其步长不为 1，需要将二者转成一致\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)  # 输出 + 输入\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    实现主 module: ResNet-18\n",
    "    ResNet 包含多个 layer, 每个 layer 又包含多个 residual block (上面实现的类)\n",
    "    因此, 用 ResidualBlock 实现 Residual 部分，用 _make_layer 函数实现 layer\n",
    "    \"\"\"\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        # 最开始的操作\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # 四个 layer， 对应 2， 3， 4， 5 层， 每层有两个 residual block\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        # 最后的全连接，分类时使用\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        构建 layer, 每一个 layer 由多个 residual block 组成\n",
    "        在 ResNet 中，每一个 layer 中只有两个 residual block\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0:  # 第一个是输入的 stride\n",
    "                layers.append(block(self.inchannel, channels, stride))\n",
    "            else:    # 后面的所有 stride，都置为 1\n",
    "                layers.append(block(channels, channels, 1))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)  # 时序容器。Modules 会以他们传入的顺序被添加到容器中。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 最开始的处理\n",
    "        out = self.conv1(x)\n",
    "        # 四层 layer\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        # 全连接 输出分类信息\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def Accuracy(testloader, net, device):\n",
    "    # 使用测试数据测试网络\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # 将输入和目标在每一步都送入GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "def run(device):\n",
    "    # 超参数设置\n",
    "    # 批处理尺寸(batch_size)\n",
    "    BATCH_SIZE = 128\n",
    "    # 学习率\n",
    "    LR = 0.1\n",
    "    # 加载数据集\n",
    "    # 标准化为范围在[-1, 1]之间的张量\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    # 训练集\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform)  # 训练数据集\n",
    "    # 生成一个个batch进行批训练，组成batch的时候顺序打乱取\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # 测试集\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "    # Cifar-10的标签\n",
    "    # 建立神经网络\n",
    "    net = ResNet(ResidualBlock).to(device)\n",
    "    # 定义损失函数\n",
    "    criterion = nn.CrossEntropyLoss()  # 损失函数为交叉熵，多用于多分类问题\n",
    "    # 优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "    optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "    # 训练网络\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # 取数据\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 将输入和目标在每一步都送入GPU\n",
    "            # 将梯度置零\n",
    "            optimizer.zero_grad()\n",
    "            # 训练\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels).to(device)\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 优化\n",
    "            # 统计数据\n",
    "            running_loss += loss.item()\n",
    "            if i >= 0:  # 每 2000 张图片，打印一次\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                acc = Accuracy(testloader, net, device)\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('/Users/liuchu/Downloads/net1 (1).pth',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9ffa661e886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m transform = transforms.Compose([transforms.ToTensor(),\n\u001b[1;32m      2\u001b[0m                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[0;32m---> 69\u001b[0;31m                                ' You can use download=True to download it')\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 84 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.36"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy(testloader,net,'cpu')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-92be9ca84a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img.numpy(),(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "for i,(images,_) in enumerate(testloader):\n",
    "    print(i)\n",
    "    print(images.numpy().shape)\n",
    "    v = torchvision.utils.make_grid(images)\n",
    "    print(type(v))\n",
    "    imshow(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/liuchu/Desktop/clean/clean/0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.txt\n",
      "clean\n"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(path):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235M\t/Users/liuchu/Desktop/clean\r\n"
     ]
    }
   ],
   "source": [
    "!du -d=1 -h /Users/liuchu/Desktop/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_PIL: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x123D56470>\n",
      "img_PIL: <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "img_PIL: (32, 32, 3)\n",
      "img_PIL: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img_PIL = Image.open(path)#读取数据\n",
    "\n",
    "print(\"img_PIL:\",img_PIL)\n",
    "\n",
    "print(\"img_PIL:\",type(img_PIL))\n",
    "\n",
    "#将图片转换成np.ndarray格式\n",
    "img_PIL = np.array(img_PIL)\n",
    "print(\"img_PIL:\",img_PIL.shape)\n",
    "\n",
    "print(\"img_PIL:\",type(img_PIL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4d504b88f498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimshow1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_PIL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-0085bc4bd2c8>\u001b[0m in \u001b[0;36mimshow1\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimshow1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow1(img_PIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow1(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img,(1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_PIL = Image.open(path)\n",
    "plt.imshow(img_PIL)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor(), # 将图片转换为Tensor,归一化至[0,1]\n",
    " transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]) # 标准化至[-1,1]\n",
    "])\n",
    "\n",
    "#定义自己的数据集合\n",
    "class FlameSet(data.Dataset):\n",
    "    def __init__(self,root):\n",
    "      # 所有图片的绝对路径\n",
    "        last_dir = os.path.abspath(os.path.join(root,'..'))\n",
    "        label_path = os.path.join(last_dir,'label.txt')\n",
    "        self.mapping = {}\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path,'r') as f:\n",
    "                for s in f.readlines():\n",
    "                    s = s.strip()\n",
    "                    v1,v2 = s.split(' ')\n",
    "                    v2 = int(v2)\n",
    "                    self.mapping[v1] = v2\n",
    "        imgs=os.listdir(root)\n",
    "        self.imgs=[os.path.join(root,k) for k in imgs]\n",
    "        self.transforms=transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        name = os.path.basename(img_path)\n",
    "        pil_img = Image.open(img_path)\n",
    "        if self.transforms:\n",
    "            data = self.transforms(pil_img)\n",
    "        else:\n",
    "            pil_img = np.asarray(pil_img)\n",
    "            data = torch.from_numpy(pil_img)\n",
    "        if self.mapping:\n",
    "            return data,torch.tensor(self.mapping[name])\n",
    "        else:\n",
    "            return data,name\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liuchu\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(os.path.dirname('/Users/liuchu/Desktop/../')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-554dd381e3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 72x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img.numpy(),(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "dataSet=FlameSet('/Users/liuchu/Desktop/clean/clean')\n",
    "dataloader = torch.utils.data.DataLoader(dataSet, batch_size=64, shuffle=False)\n",
    "\n",
    "for i,images in enumerate(dataloader):\n",
    "    print(i)\n",
    "    print(images.numpy().shape)\n",
    "    v = torchvision.utils.make_grid(images)\n",
    "    print(type(v))\n",
    "    imshow(v)\n",
    "    break\n",
    "# v = torchvision.utils.make_grid(dataSet[100])\n",
    "# imshow(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3], but got 3-dimensional input of size [3, 32, 32] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3e93fb91187f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net1 (1).pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3d06c2be002f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# 最开始的处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;31m# 四层 layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3], but got 3-dimensional input of size [3, 32, 32] instead"
     ]
    }
   ],
   "source": [
    "net = torch.load('net1 (1).pth',map_location=torch.device('cpu'))\n",
    "outputs = net(dataSet[0])\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 8, 6, 9, 5, 4, 8, 4, 2, 4, 6, 2, 3, 4, 6, 3, 7, 5, 4, 1, 5, 2, 5,\n",
      "        1, 3, 0, 0, 8, 2, 2, 4, 5, 0, 3, 6, 6, 3, 4, 4, 5, 1, 7, 5, 3, 9, 2, 6,\n",
      "        7, 5, 1, 4, 5, 6, 7, 0, 1, 9, 3, 5, 6, 5, 1, 4])\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        images,label = data\n",
    "        # 将输入和目标在每一步都送入GPU\n",
    "        images = images.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        print((label == predicted).sum().item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.32"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy(dataloader,net,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_data_set = FlameSet('/Users/liuchu/Desktop/B')\n",
    "B_loader = torch.utils.data.DataLoader(B_data_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_data_set = FlameSet('/Users/liuchu/Desktop/A')\n",
    "A_loader = torch.utils.data.DataLoader(A_data_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['飞机', '汽车' ,'鸟', '猫','鹿','狗', '青蛙', '马','船','卡车']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img.numpy(),(1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 7, 5, 6, 6, 5, 6, 8, 2, 3, 4, 4, 0, 8, 1, 6, 5, 0, 8, 0, 3, 3, 1,\n",
      "        8, 5, 5, 8, 1, 5, 5, 0, 1, 8, 5, 4, 4, 6, 1, 7, 8, 9, 1, 7, 1, 2, 9, 0,\n",
      "        5, 6, 9, 5, 5, 9, 3, 6, 0, 7, 2, 6, 7, 1, 1, 6])\n",
      "tensor([7, 3, 0, 7, 2, 8, 1, 4, 3, 7, 9, 4, 7, 7, 3, 0, 7, 9, 8, 1, 6, 0, 9, 2,\n",
      "        8, 4, 6, 7, 6, 8, 8, 9, 2, 5, 8, 0, 3, 5, 1, 2, 2, 5, 5, 1, 4, 7, 3, 2,\n",
      "        9, 9, 4, 5, 7, 7, 0, 1, 6, 6, 5, 5, 3, 2, 2, 5])\n",
      "tensor([4, 3, 9, 2, 2, 5, 3, 5, 7, 9, 2, 9, 6, 3, 0, 5, 0, 4, 1, 4, 4, 4, 9, 4,\n",
      "        8, 1, 9, 7, 8, 4, 5, 2, 4, 3, 9, 4, 6, 8, 6, 0, 3, 4, 3, 7, 1, 1, 5, 4,\n",
      "        0, 8, 1, 0, 8, 9, 6, 3, 5, 0, 7, 8, 7, 3, 8, 6])\n",
      "tensor([6, 5, 7, 2, 3, 9, 6, 1, 6, 8, 3, 2, 1, 9, 8, 4, 7, 0, 8, 4, 7, 4, 4, 9,\n",
      "        3, 5, 5, 7, 0, 9, 8, 3, 0, 7, 0, 5, 6, 4, 0, 6, 8, 2, 9, 5, 5, 3, 8, 6,\n",
      "        7, 5, 4, 6, 5, 4, 2, 1, 1, 8, 2, 9, 8, 3, 6, 0])\n",
      "tensor([0, 5, 3, 6, 3, 5, 1, 7, 0, 1, 2, 4, 9, 5, 7, 4, 4, 1, 0, 2, 8, 7, 9, 6,\n",
      "        9, 5, 3, 9, 6, 1, 8, 8, 6, 0, 2, 1, 5, 9, 4, 5, 1, 2, 7, 8, 0, 1, 0, 3,\n",
      "        6, 9, 8, 5, 2, 8, 6, 1, 2, 9, 8, 4, 4, 0, 7, 0])\n",
      "tensor([7, 4, 5, 1, 2, 1, 5, 3, 9, 9, 5, 8, 8, 5, 5, 9, 9, 0, 6, 8, 5, 3, 6, 0,\n",
      "        6, 3, 1, 9, 9, 0, 2, 7, 6, 4, 5, 1, 7, 7, 1, 6, 8, 0, 9, 3, 4, 1, 8, 6,\n",
      "        0, 4, 3, 4, 9, 7, 6, 1, 8, 2, 6, 0, 8, 3, 8, 6])\n",
      "tensor([6, 7, 3, 2, 0, 8, 9, 8, 1, 9, 0, 5, 6, 7, 0, 6, 5, 7, 0, 4, 4, 9, 0, 2,\n",
      "        3, 0, 3, 0, 8, 6, 3, 5, 7, 5, 6, 3, 1, 9, 8, 0, 0, 8, 1, 6, 6, 7, 9, 7,\n",
      "        9, 6, 4, 3, 7, 6, 7, 7, 6, 7, 6, 7, 9, 8, 2, 5])\n",
      "tensor([8, 8, 5, 5, 8, 3, 9, 1, 1, 9, 2, 5, 7, 2, 6, 4, 0, 2, 7, 5, 6, 6, 5, 6,\n",
      "        5, 7, 2, 9, 1, 1, 9, 8, 6, 1, 7, 4, 6, 4, 8, 4, 7, 6, 9, 4, 0, 2, 1, 0,\n",
      "        3, 6, 1, 4, 2, 5, 8, 0, 0, 0, 6, 4, 7, 9, 9, 7])\n",
      "tensor([1, 6, 9, 1, 4, 0, 0, 2, 0, 2, 5, 2, 1, 6, 0, 9, 0, 2, 6, 8, 6, 9, 7, 4,\n",
      "        8, 8, 5, 7, 5, 5, 3, 3, 5, 0, 2, 7, 9, 9, 5, 8, 4, 8, 3, 7, 5, 8, 9, 4,\n",
      "        1, 8, 9, 8, 2, 8, 1, 3, 2, 4, 5, 8, 0, 2, 3, 1])\n",
      "tensor([8, 9, 6, 4, 7, 8, 8, 6, 5, 3, 1, 7, 5, 7, 2, 9, 4, 4, 7, 0, 6, 5, 0, 0,\n",
      "        6, 5, 8, 5, 7, 1, 0, 6, 5, 9, 0, 8, 8, 3, 3, 4, 7, 7, 5, 4, 0, 7, 1, 9,\n",
      "        3, 1, 3, 0, 4, 1, 5, 1, 6, 9, 2, 4, 2, 3, 7, 0])\n",
      "tensor([5, 9, 8, 6, 1, 8, 5, 3, 7, 0, 6, 8, 7, 0, 1, 9, 6, 8, 6, 0, 1, 0, 5, 5,\n",
      "        3, 0, 7, 2, 0, 9, 8, 4, 9, 7, 4, 7, 3, 1, 9, 6, 5, 9, 1, 2, 9, 0, 1, 6,\n",
      "        7, 0, 6, 7, 1, 7, 1, 6, 4, 7, 3, 4, 5, 1, 8, 4])\n",
      "tensor([5, 6, 7, 1, 7, 4, 6, 3, 5, 4, 9, 9, 2, 0, 4, 0, 4, 9, 9, 3, 2, 0, 5, 3,\n",
      "        6, 9, 7, 5, 6, 1, 0, 2, 0, 0, 2, 1, 8, 5, 9, 8, 7, 6, 1, 8, 1, 4, 6, 2,\n",
      "        4, 7, 6, 1, 3, 9, 1, 4, 8, 7, 5, 5, 2, 9, 7, 2])\n",
      "tensor([6, 6, 8, 8, 3, 0, 9, 4, 9, 2, 5, 5, 2, 8, 3, 9, 1, 7, 2, 2, 4, 4, 5, 8,\n",
      "        1, 4, 7, 3, 5, 9, 7, 4, 7, 8, 5, 8, 5, 0, 4, 2, 3, 7, 1, 8, 0, 8, 5, 1,\n",
      "        4, 1, 7, 7, 4, 8, 5, 4, 6, 0, 9, 6, 9, 1, 2, 5])\n",
      "tensor([6, 5, 2, 8, 9, 0, 4, 2, 3, 8, 8, 9, 8, 8, 9, 5, 7, 9, 1, 4, 8, 1, 1, 2,\n",
      "        2, 6, 2, 1, 6, 2, 3, 5, 4, 0, 5, 9, 2, 2, 1, 0, 6, 9, 7, 7, 0, 4, 5, 6,\n",
      "        7, 5, 7, 5, 4, 4, 1, 9, 6, 4, 3, 6, 9, 2, 4, 7])\n",
      "tensor([2, 4, 8, 8, 7, 3, 9, 6, 6, 5, 3, 3, 3, 3, 2, 7, 7, 8, 8, 8, 2, 6, 6, 1,\n",
      "        5, 3, 1, 0, 9, 3, 8, 2, 0, 5, 3, 1, 6, 0, 3, 0, 4, 1, 9, 1, 9, 9, 1, 2,\n",
      "        1, 5, 5, 3, 6, 0, 3, 0, 7, 0, 2, 7, 7, 3, 1, 1])\n",
      "tensor([7, 0, 9, 7, 8, 4, 9, 7, 8, 8, 9, 1, 0, 8, 0, 5, 0, 2, 5, 8, 1, 6, 2, 1,\n",
      "        4, 5, 3, 7, 5, 9, 4, 6, 3, 3, 8, 6, 8, 9, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i,alist in enumerate(A_loader):\n",
    "#     print(alist)\n",
    "    images,names = alist\n",
    "    \n",
    "#     print(i)\n",
    "#     print(images.numpy().shape)\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(predicted)\n",
    "#     v = torchvision.utils.make_grid(images)\n",
    "#     print(type(v))\n",
    "#     imshow(v)\n",
    "    hh = defaultdict(list)\n",
    "    for img,pre,name in zip(images,predicted,names):\n",
    "#         imshow(img)\n",
    "        label = arr[pre.item()]\n",
    "        hh[label].append([img,name]) \n",
    "        result.append([name,pre.item()])\n",
    "#     print(hh)\n",
    "#     for k in hh:\n",
    "#         imlist = [v[0] for v in hh[k]]\n",
    "#         namelist = [v[1] for v in hh[k]]\n",
    "#         imshow(torchvision.utils.make_grid(imlist))\n",
    "#         print(k,namelist)\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for name,pre in result:\n",
    "#     print(name,pre)\n",
    "    text.append('{} {}'.format(name,str(pre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/liuchu/Desktop/result.txt','w') as f:\n",
    "    f.write('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "clean/clean\n"
     ]
    }
   ],
   "source": [
    "for dir_name,file_name in [('A','A.txt'),('B','B.txt'),('clean/clean','clean.txt')]:\n",
    "    A_data_set = FlameSet('/Users/liuchu/Desktop/{}'.format(dir_name))\n",
    "    A_loader = torch.utils.data.DataLoader(A_data_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    result = []\n",
    "    for i,alist in enumerate(A_loader):\n",
    "    #     print(alist)\n",
    "        images,names = alist\n",
    "\n",
    "    #     print(i)\n",
    "    #     print(images.numpy().shape)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(predicted)\n",
    "    #     v = torchvision.utils.make_grid(images)\n",
    "    #     print(type(v))\n",
    "    #     imshow(v)\n",
    "        hh = defaultdict(list)\n",
    "        for img,pre,name in zip(images,predicted,names):\n",
    "    #         imshow(img)\n",
    "            label = arr[pre.item()]\n",
    "            hh[label].append([img,name]) \n",
    "            result.append([name,pre.item()])\n",
    "    #     print(hh)\n",
    "    #     for k in hh:\n",
    "    #         imlist = [v[0] for v in hh[k]]\n",
    "    #         namelist = [v[1] for v in hh[k]]\n",
    "    #         imshow(torchvision.utils.make_grid(imlist))\n",
    "    #         print(k,namelist)\n",
    "    #     if i > 5:\n",
    "    #         break\n",
    "    text = []\n",
    "    for name,pre in result:\n",
    "    #     print(name,pre)\n",
    "        text.append('{} {}'.format(name,str(pre)))\n",
    "    with open('/Users/liuchu/Desktop/{}'.format(file_name),'w') as f:\n",
    "        f.write('\\n'.join(text))\n",
    "    print(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
